{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12537ef",
   "metadata": {
    "id": "b12537ef"
   },
   "source": [
    "# Scrapy\n",
    "- 웹사이트에서 데이터 수집을 위한 오픈소스 파이썬 프레임워크\n",
    "- 멀티스레딩으로 데이터 수집\n",
    "- gmarket 상품데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8df8471",
   "metadata": {
    "collapsed": true,
    "id": "c8df8471",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\user\\anaconda3\\lib\\site-packages (2.11.1)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (23.10.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (42.0.5)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (1.8.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (24.0.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (2.1.2)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (0.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (69.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (23.2)\n",
      "Requirement already satisfied: tldextract in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (3.2.0)\n",
      "Requirement already satisfied: lxml>=4.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (5.2.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from protego>=0.1.15->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=16.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (24.2.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\user\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: automat>=0.8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=22.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (1.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (4.11.0)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (3.7)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (2.32.2)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (3.13.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "#install scrapy\n",
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecf9f5af-145d-428e-9840-ee627b1db8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy, requests\n",
    "from scrapy.http import TextResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0332cd8-37a0-4f19-9a84-e64a565827d6",
   "metadata": {
    "id": "e90ab90f"
   },
   "source": [
    "## 1. make project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cfc2268-550c-43b8-b5fc-eb28abd0976f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'news', using template directory 'C:\\Users\\User\\anaconda3\\Lib\\site-packages\\scrapy\\templates\\project', created in:\n",
      "    C:\\Users\\User\\aivle\\Web crawling\\notebooks_code\\news\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd news\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab608ad-302a-44bf-8f41-75fe6ff61be7",
   "metadata": {
    "id": "cac6776b",
    "tags": []
   },
   "source": [
    "- scrapy structure\n",
    "    - items : 데이터의 모양 정의\n",
    "    - middewares : 수집할때 header 정보와 같은 내용을 설정\n",
    "    - pipelines : 데이터를 수집한 후에 코드를 실행\n",
    "    - settings : robots.txt 규칙, 크롤링 시간 텀등을 설정\n",
    "    - spiders : 크롤링 절차를 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c2301-9562-46fe-aece-481dd73b88c9",
   "metadata": {
    "id": "5f97e6dc",
    "tags": []
   },
   "source": [
    "## 2. xpath\n",
    "- link, contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aac82cb",
   "metadata": {
    "id": "3aac82cb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scrapy, requests\n",
    "from scrapy.http import TextResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82d156f8-879c-4449-af87-d095cfa5a200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200 https://news.daum.net/>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://news.daum.net/'\n",
    "response=requests.get(url)\n",
    "response=TextResponse(response.url,body=response.text,encoding='utf-8')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e62e523-22eb-4ee2-8ef6-f7acc806f6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " ['https://v.daum.net/v/20240923151053270',\n",
       "  'https://v.daum.net/v/20240923150750118',\n",
       "  'https://v.daum.net/v/20240923145528413'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = '/html/body/div[2]/main/section/div/div[1]/div[1]/ul/li/div/div/strong/a/@href'\n",
    "links = response.xpath(selector).extract()\n",
    "len(links), links[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0671ca18-ac82-4446-a290-67f5c663a51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200 https://v.daum.net/v/20240923151053270>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link=links[0]\n",
    "response=requests.get(link)\n",
    "response=TextResponse(response.url,body=response.text,encoding='utf-8')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "baec3b38-6f41-4e43-8d5d-f0024ef78b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"전국 주택 2가구 중 1가구 이상은 '준공 20년 이상'\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#상세페이지에서 제목 가져오기. 저번엔 url을 건드리지않고 뒤에 .text로 가져왔으나 이번엔 url에 text()로 가져오고 .extract()로 깔끔하게 제목만 가져오기\n",
    "title=response.xpath('//*[@id=\"mArticle\"]/div[1]/h3/text()')[0].extract()\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66594e9-822f-412b-b6ac-5e6d192d943a",
   "metadata": {},
   "source": [
    "## 3. items.py\n",
    "- Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61811a6-5e4b-4599-830a-071490b8473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load news/news/items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da0a8805-28ee-4deb-9281-ecc84b0b68bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting news/news/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile news/news/items.py\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class NewsItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36c20c-a58d-4f60-8ed4-b13178e174d7",
   "metadata": {
    "id": "4d0aa7e1",
    "tags": []
   },
   "source": [
    "## 4. spider.py\n",
    "- wirte crawling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cdb3e0db-4ff6-4ec7-aa24-b8ae37feff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting news/news/spiders/spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile news/news/spiders/spider.py\n",
    "import scrapy\n",
    "class news.items importNewsItem\n",
    "\n",
    "class NewsSpider(scrapy.Spider):\n",
    "    name='news'\n",
    "    allow_domain=['daum.net']\n",
    "    start_urls=['https://news.daum.net']\n",
    "\n",
    "   def parse(self, response):\n",
    "    selector = '/html/body/div[2]/main/section/div/div[1]/div[1]/ul/li/div/div/strong/a/@href'\n",
    "    links = response.xpath(selector).extract()\n",
    "    for link in links:\n",
    "        yield scrapy.Request(link, callback=self.parse_content):\n",
    "\n",
    "  def parse_content(self, response):\n",
    "    item = NewsItem()\n",
    "    item['link'] = response.url\n",
    "    item['title'] = response.xpath('//*[@id=\"mArticle\"]/div[1]/h3/text()')[0].extract()\n",
    "    yield item\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d4d2e-1978-4d8c-ac50-0ca52803c670",
   "metadata": {
    "id": "bd2e1c16",
    "tags": []
   },
   "source": [
    "## 5. run scrapy\n",
    "- news 디렉토리에서 아래의 커멘드 실행\n",
    "- scrapy crawl news -o news.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8811644a-184c-46f7-b2f3-ceda17dac532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\aivle\\\\Web_crawling\\\\notebooks_code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a7e2d002-3666-4695-81f1-971c43d8abbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: BEA2-02D7\n",
      "\n",
      " C:\\Users\\User\\aivle\\Web crawling\\notebooks_code 디렉터리\n",
      "\n",
      "2024-09-23  오후 03:48    <DIR>          .\n",
      "2024-09-23  오후 01:54    <DIR>          ..\n",
      "2024-09-23  오후 02:58    <DIR>          .ipynb_checkpoints\n",
      "2024-09-20  오전 11:18           133,678 01_requests_naver_stock.ipynb\n",
      "2024-09-20  오후 12:05             2,545 02_requests_daum_exchange.ipynb\n",
      "2024-09-20  오후 02:11            86,782 03_rest_api.ipynb\n",
      "2024-09-20  오후 03:12            14,640 04_requests_zigbang.ipynb\n",
      "2024-09-20  오후 04:14            11,017 05_html.ipynb\n",
      "2024-09-20  오후 04:39             7,616 06_css_selector.ipynb\n",
      "2024-09-21  오전 12:26             6,735 07_naver_relational_keywords.ipynb\n",
      "2024-09-12  오후 07:58             2,041 08_gmarket.ipynb\n",
      "2024-09-23  오후 02:50            15,537 09_selenium.ipynb\n",
      "2024-09-23  오후 03:00             1,465 10_xpath.ipynb\n",
      "2024-09-06  오후 03:50             2,189 11_iterator_generator.ipynb\n",
      "2024-09-23  오후 03:48            14,131 12_scrapy.ipynb\n",
      "2024-09-23  오후 01:52    <DIR>          chromedriver-win64\n",
      "2024-09-23  오후 03:04    <DIR>          news\n",
      "              12개 파일             298,376 바이트\n",
      "               5개 디렉터리  164,479,803,392 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327723e3-581e-46c0-8312-a30e41212061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "04_scrapy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
